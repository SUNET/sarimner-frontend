#!/usr/bin/env python3
#
# Copyright 2019 SUNET. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification, are
# permitted provided that the following conditions are met:
#
#    1. Redistributions of source code must retain the above copyright notice, this list of
#       conditions and the following disclaimer.
#
#    2. Redistributions in binary form must reproduce the above copyright notice, this list
#       of conditions and the following disclaimer in the documentation and/or other materials
#       provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY SUNET ``AS IS'' AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
# FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL SUNET OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
# ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# The views and conclusions contained in the software and documentation are those of the
# authors and should not be interpreted as representing official policies, either expressed
# or implied, of SUNET.
#
# Author: Fredrik Thulin <fredrik@thulin.net>
#

"""
Start a frontend instance, with locking to not start simultaneously with another instance.

This is intended to help a systemd based frontend load balancer start more efficiently.
Systemd starts all frontend services in parallel, which causes a _lot_ of docker containers
to start simultaneously. This is really poor for performance, so better let the instances
compete for an exclusive lock and start one at a time.

Since systemd will follow the main process, that process should be the one that runs
docker-compose up in the foreground. This script therefor starts with forking off a controller
process, that will acquire the lock. When it acquires the lock, it will send a signal back
to the main process to start the frontend instance, and then start the network container
initalisation before releasing the lock again.


    PARENT                                   CHILD
       |
      fork                                     |
       |                        -------- acquire lock
       |                        |              |
       |                   fail for 1min    success
       |                        |              |
       |                 terminate both        |
       |                parent and child       |
       |               using an 'exit' cmd     |
       |                                       |
       | <------- cmd 'compose_up' ------------|
       |                                       |
     execv docker-compose                      |
                                               |
                                        run network init
                                               |
                                         release lock
                                               |
                                             exit

Ideas for further improvement:

  - Start the network initialisation when the haproxy container is present
  - Hang around watching the containers. If they malfunction, kill the service.
"""
import fcntl
import logging
import os
import signal
import sys
from fcntl import LOCK_EX, LOCK_NB
from logging.handlers import SysLogHandler
from multiprocessing import Process, Queue

import argparse
import datetime
import subprocess
import time

_defaults = {'syslog': True,
             'debug': True,
             'lockfile': '/var/lock/start-frontend',
             'lock_retry': 60,
             'compose_bin': '/usr/local/bin/docker-compose'
             }

def parse_args(defaults=None):
    if defaults is None:
        defaults = _defaults

    parser = argparse.ArgumentParser(description = 'SÃ¤rimner instance starter',
                                     add_help = True,
                                     formatter_class = argparse.ArgumentDefaultsHelpFormatter,
    )

    # Positional arguments
    parser.add_argument('basedir',
                        metavar='DIR',
                        help='Base directory (should have subdir \'scripts\')',
    )

    parser.add_argument('name',
                        metavar='NAME',
                        help='Name of instance',
    )

    parser.add_argument('compose_file',
                        metavar='FILE',
                        help='Docker Compose file to \'up\'',
    )

    # Optional arguments
    parser.add_argument('--debug',
                        dest = 'debug',
                        action = 'store_true', default=defaults['debug'],
                        help = 'Enable debug operation',
    )
    parser.add_argument('--syslog',
                        dest = 'syslog',
                        action = 'store_true', default=defaults['syslog'],
                        help = 'Enable syslog output',
    )
    parser.add_argument('--lockfile',
                        dest = 'lockfile',
                        type=str, default=defaults['lockfile'],
                        help = 'Lock file to acquire',
    )
    parser.add_argument('--lock_retry',
                        dest = 'lock_retry',
                        type=int, default=defaults['lock_retry'],
                        help = 'Seconds to wait for a lock before giving up',
    )
    parser.add_argument('--compose_bin',
                        dest = 'compose_bin',
                        type=str, default=defaults['compose_bin'],
                        help = 'Full path to docker-compose binary',
    )
    args = parser.parse_args()
    return args


def get_logger(myname, args, logger_in=None):
    # initialize various components
    if logger_in:
        logger = logger_in
    else:
        # This is the root log level
        level = logging.INFO
        if args.debug:
            level = logging.DEBUG
        logging.basicConfig(level = level, stream = sys.stderr,
                            format='%(asctime)s: %(name)s: %(threadName)s %(levelname)s %(message)s')
        logger = logging.getLogger(myname)
        # If stderr is not a TTY, change the log level of the StreamHandler (stream = sys.stderr above) to WARNING
        if not sys.stderr.isatty() and not args.debug:
            for this_h in logging.getLogger('').handlers:
                this_h.setLevel(logging.WARNING)
    if args.syslog:
        syslog_h = logging.handlers.SysLogHandler()
        formatter = logging.Formatter('%(name)s: %(levelname)s %(message)s')
        syslog_h.setFormatter(formatter)
        logger.addHandler(syslog_h)

    return logger


def manager_process(script, args, q, logger):
    """
    This is the child process that will acquire the exclusive lock, signal the parent
    to start docker-compose, initialise the container networking and then release the lock.
    """
    logger.debug('Starting manager process for frontend instance {!r}'.format(args.name))
    retry = args.lock_retry
    with open(args.lockfile, 'w') as lock_fd:
        logger.info('Acquiring lock for frontend instance {!r}'.format(args.name))

        locked = False
        while not locked:
            try:
                fcntl.lockf(lock_fd, LOCK_EX | LOCK_NB)
                locked = True
            except OSError:
                retry -= 1
                #logger.debug('Did not get a lock (retry {})'.format(retry))
                time.sleep(1)
                if retry and (retry % 10 == 0):
                    logger.info('Frontend instance {!r} still waiting for lock ({} seconds left)'.format(
                        args.name, retry
                    ))
            if not locked and not retry:
                logger.error('Failed to acquire lock in {} seconds - exiting'.format(args.lock_retry))
                q.put('exit')
                return False

        logger.info('Lock acquired (frontend instance {!r})'.format(args.name))
        q.put('compose_up')

        logger.info('Sleeping three seconds')
        time.sleep(3)
        logger.info('Starting {} {}'.format(script, args.name))
        t1 = datetime.datetime.now()
        res = subprocess.run([script, args.name])
        t2 = datetime.datetime.now()
        logger.info('Frontend instance {!r} network configured in {:.2f} seconds'.format(
            args.name, (t2 - t1).total_seconds()))
        logger.debug('Network configuration script result: {}'.format(res))
        q.put('exit')
    logger.info('Releasing lock and exiting (frontend instance {!r} finished)'.format(args.name))


def main(myname='start-frontend', args=None, logger_in=None):
    if not args:
        args = parse_args()

    logger = get_logger(myname, args, logger_in)

    script = os.path.join(args.basedir, 'scripts', 'configure-container-network')
    if not os.path.isfile(script):
        logger.error('Incorrect basedir, {} not found'.format(script))
        return False

    q = Queue()
    p = Process(target=manager_process, args=(script, args, q, logger.getChild('manager'),))
    p.start()
    logger.debug('Started process {}'.format(p))

    # Ignore the SIGCHLD signal when the manager process terminates, to not get a zombie process entry...
    signal.signal(signal.SIGCHLD, signal.SIG_IGN)

    while True:
        logger.debug('Waiting for commands from the manager process...')
        cmd = q.get()
        logger.debug('Got command {!r}'.format(cmd))
        if cmd == 'compose_up':
            logger.info('Starting frontend instance {!r}'.format(args.name))
            os.execv(args.compose_bin, [args.compose_bin,
                                        '-f', args.compose_file,
                                        'up',
                                        '--force-recreate',
                                        '--no-deps',
                                        ])
        elif cmd == 'exit':
            logger.info('Received exit command')
            return True
        else:
            logger.info('Received command {!r} - exiting'.format(cmd))
            return False


if __name__ == '__main__':
    try:
        progname = os.path.basename(sys.argv[0])
        res = main(progname)
        if res is True:
            sys.exit(0)
        if res is False:
            sys.exit(1)
        sys.exit(int(res))
    except KeyboardInterrupt:
        sys.exit(0)
